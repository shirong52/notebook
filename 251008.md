---


---

<h2 id="微分几何">微分几何</h2>
<p>13:30-14:30</p>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV1qF411t72r?spm_id_from=333.788.videopod.episodes&amp;vd_source=96ef48634663967d0116e79abff26934">微分几何入门与广义相对论-梁灿彬老师（全集） 高清修复1080p_哔哩哔哩_bilibili</a><br>
<a href="https://www.bilibili.com/video/BV1qF411t72r?spm_id_from=333.788.videopod.episodes&amp;vd_source=96ef48634663967d0116e79abff26934&amp;p=2">2.开子集、拓扑空间、开球与通常拓扑、诱导拓扑、同胚homeomorphism_哔哩哔哩_bilibili</a></p>
</blockquote>
<h2 id="代码随想录">代码随想录</h2>
<p>14:30-18:30<br>
中间穿插了帮老师扫描答案，约50分钟</p>
<h3 id="、复原ip地址">1、复原IP地址</h3>
<ul>
<li>startIndex是分割线</li>
<li>切割时是左闭右闭区间</li>
<li>注意IP地址要加上标点，然后在递归时位置移动要算上标点的位置</li>
<li>以标点数量作为终止条件进行判断，此时标点数量满足要求但是还未判断IP的最后一段</li>
<li>IP地址合法条件：均为数字；数字不大于255；数字除了0本身不能以0开头（判断条件为s[start] == “0” and start != end）</li>
</ul>
<h3 id="、子集问题">2、子集问题</h3>
<ul>
<li>每进入一层递归，都要收获当层的结果，并不同于之前在叶子节点收获结果</li>
<li>注意收获结果和终止条件判断所放置的位置，反了可能导致无法收获到最后一个元素</li>
</ul>
<h3 id="、子集ii">3、子集II</h3>
<ul>
<li>进行去重操作，首先进行排序，然后可以通过used数组来记录，树层去重</li>
<li>这里注意判断条件是大于statIndex，若只是大于0的话会导致往前判断</li>
</ul>
<h3 id="、递增子序列">4、递增子序列</h3>
<ul>
<li>取值的时候注意两点：树层上重复的元素不要再取；树枝上取的元素要求大于已取数组最右边的元素</li>
<li>子集类问题可以不用写终止条件，因为for循环里面其实相当于有个终止</li>
<li>注意是使用continue而不是break整个循环，因为还有需要取的元素</li>
<li>set用于记录当前层递归里元素不要取重复元素，每一层都定义了一个set用于记录当层，故而不需要对set进行回溯，与path有区别</li>
</ul>
<h3 id="、全排列">5、全排列</h3>
<ul>
<li>排列有顺序</li>
<li>组合类问题使用的是startIndex来避免重复取同一个元素，而排列使用used数组来避免</li>
<li>所求的集合在叶子节点下，即终止条件为path的大小与集合大小相等时可以收获</li>
<li>树的深度即为集合的大小</li>
</ul>
<h3 id="、全排列ii">6、全排列II</h3>
<ul>
<li>去重一定要记得先排序！！！！</li>
<li>对于排列问题，树层上去重和树枝上去重，都是可以的，但是树层上去重效率更高</li>
<li>如果要对树层中前一位去重，就用used[i - 1] = false，如果要对树枝前一位去重用used[i - 1] = true</li>
</ul>
<h2 id="huggingface-llm">HuggingFace LLM</h2>
<blockquote>
<p>完成饿了第三章以及相应的绿色方框的练习，我是真的棒！</p>
</blockquote>
<p>为甚么predictions直接选取最大的logits而不再经过softmax？</p>
<ul>
<li>softmax 函数是一个单调函数，即它不会改变原始值的相对顺序。也就是说，logits 中的最大值在经过 softmax 后仍然是最大值。因此，argmax(logits) 和 argmax(softmax(logits)) 的结果是相同的。</li>
<li>在许多情况下，我们只需要知道哪个类别的分数最高，而不需要具体的概率值。直接对 logits 使用 argmax 可以避免额外的 softmax 计算，从而提高效率。</li>
<li>在许多深度学习模型中，logits 是模型的最终输出，通常是一个未归一化的分数。这些分数已经反映了模型对每个类别的置信度。虽然 softmax 可以将这些分数转换为概率，但 argmax 直接作用于 logits 也能得到正确的预测类别。</li>
</ul>

