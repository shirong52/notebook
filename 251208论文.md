---


---

<h1 id="《beyond-the-cls-token-image-reranking-using-pretrained-vision-transformers》深度解读">《Beyond the CLS token: Image reranking using pretrained vision transformers》深度解读</h1>
<h2 id="研究目标与问题意义">1. 研究目标与问题意义</h2>
<h3 id="研究目标">研究目标</h3>
<ul>
<li><strong>提升图像检索的精度与可解释性</strong>：突破Vision Transformer（ViT）仅用<strong>CLS token</strong>（全局特征）的局限，利用patch token的结构相似性改进检索结果，并提供可视化的相似性解释。</li>
<li><strong>验证卷积对ViT结构学习的作用</strong>：对比不同ViT变体（Deit、CvT、Swin-T）的patch表示，证明卷积操作能让ViT学习到<strong>局部平滑且语义 discriminative</strong>的特征。</li>
<li><strong>解决视角变化下的匹配难题</strong>：提出部分最优传输（Partial Optimal Transport）处理强视角/尺度变化的图像对，提升检索鲁棒性。</li>
</ul>
<h3 id="实际问题">实际问题</h3>
<p>现有图像检索方法存在两大痛点：</p>
<ol>
<li><strong>可解释性缺失</strong>：仅用全局特征无法说明“两张图像为何相似”；</li>
<li><strong>鲁棒性不足</strong>：强视角/尺度变化下，全局特征匹配容易失效。</li>
</ol>
<h3 id="产业意义">产业意义</h3>
<ul>
<li><strong>电商领域</strong>：更精准的商品检索（如相似款式推荐）+可视化解释（如“这件衣服的领口与您搜索的相似”）提升用户体验；</li>
<li><strong>自动驾驶</strong>：视觉定位（如街景匹配）的鲁棒性提升，降低天气/视角变化带来的误差；</li>
<li><strong>边缘设备</strong>：无需重新训练的结构重排序方法，可直接部署于手机端图像检索应用。</li>
</ul>
<h2 id="模型方法与创新思路">2. 模型方法与创新思路</h2>
<p>论文核心是<strong>基于ViT的结构相似性重排序框架</strong>，包含三个关键模块：注意力相关性图、最优传输匹配、部分OT扩展。</p>
<h3 id="（1）注意力相关性图：量化patch重要性">（1）注意力相关性图：量化patch重要性</h3>
<p>为解决“哪些patch对相似性贡献最大”的问题，论文采用<strong>注意力rollout</strong>（Attention Rollout）计算patch的相关性权重。其递归公式如下：</p>
<p>\tilde{A}(l_i) = \text{norm}(A(l_i)+I) \cdot \tilde{A}(l_{i-1}) \quad (i&gt;j)A<sub>(li​)=norm(A(li​)+I)⋅A</sub>(li−1​)(i&gt;j)</p>
<p>\tilde{A}(l_0) = A(l_0)A~(l0​)=A(l0​)</p>
<p>参数解释：</p>
<ul>
<li>A(l_i)A(li​)：第ii层Transformer的注意力矩阵；</li>
<li>II：单位矩阵（补充残差连接的影响）；</li>
<li>\text{norm}norm：L2归一化；</li>
<li>\tilde{A}A~：累计所有层的注意力权重，最终得到每个patch的全局相关性分数。</li>
</ul>
<blockquote>
<p>论文指出：“注意力rollout能有效聚合各层的注意力信息，生成更准确的patch重要性图”。</p>
</blockquote>
<h3 id="（2）最优传输匹配：结构相似性计算">（2）最优传输匹配：结构相似性计算</h3>
<p>对于两张图像x_sxs​（查询）和x_txt​（候选），首先提取：</p>
<ul>
<li><strong>全局特征</strong>：CLS token {g_s, g_t} \in \mathbb{R}^D{gs​,gt​}∈RD；</li>
<li><strong>局部特征</strong>：patch token {f_s, f_t} \in \mathbb{R}^{hw \times D}{fs​,ft​}∈Rhw×D（h=w=7h=w=7为patch网格大小）。</li>
</ul>
<p>然后计算：</p>
<ul>
<li><strong>全局相似度</strong>：S_{\text{global}} = \cos(g_s, g_t)Sglobal​=cos(gs​,gt​)；</li>
<li><strong>结构相似度矩阵</strong>：S_{\text{struct}} = \cos(f_s, f_t) \in \mathbb{R}^{hw \times hw}Sstruct​=cos(fs​,ft​)∈Rhw×hw（每个元素表示两个patch的余弦相似性）。</li>
</ul>
<p>为找到最优的patch匹配方式，论文采用<strong>Sinkhorn算法</strong>（最优传输的快速求解方法）：</p>
<p>T = \text{Sinkhorn}(S_{\text{struct}}, \mu_s, \mu_t)T=Sinkhorn(Sstruct​,μs​,μt​)</p>
<p>参数解释：</p>
<ul>
<li>\mu_s/\mu_tμs​/μt​：注意力rollout得到的patch重要性分布（边际分布）；</li>
<li>TT：最优匹配矩阵（表示x_sxs​的patch与x_txt​的patch的匹配概率）。</li>
</ul>
<p>最终结构相似度为：</p>
<p>S_{\text{struct}}^{\text{total}} = \sum_{i,j} T_{i,j} \cdot S_{\text{struct},i,j}Sstructtotal​=i,j∑​Ti,j​⋅Sstruct,i,j​</p>
<h3 id="（3）部分最优传输：处理视角变化">（3）部分最优传输：处理视角变化</h3>
<p>针对强视角/尺度变化（如Stanford Online Products数据集），论文扩展标准OT为<strong>部分OT</strong>：</p>
<ol>
<li><strong>添加dummy点</strong>：对两张图像各添加一个虚拟patch，其质量为1-s1−s（ss为匹配比例，如s=0.9s=0.9表示仅匹配90%的patch）；</li>
<li><strong>扩展成本矩阵</strong>：将虚拟patch的相似性设为0，形成新矩阵\bar{C}Cˉ；</li>
<li><strong>求解扩展OT</strong>：\bar{T} = \text{Sinkhorn}(\bar{C}, \bar{\mu}_s, \bar{\mu}_t)Tˉ=Sinkhorn(Cˉ,μˉ​s​,μˉ​t​)，其中\bar{\mu}μˉ​包含虚拟patch的分布。</li>
</ol>
<blockquote>
<p>论文实验显示：当s=0.9s=0.9时，SOP数据集的P@1提升0.57%（从79.42到80.02）。</p>
</blockquote>
<h3 id="方法对比">方法对比</h3>
<p>方法</p>
<p>核心特征来源</p>
<p>匹配方式</p>
<p>视角鲁棒性</p>
<p>可解释性</p>
<p>DIML（ResNet）</p>
<p>CNN特征图</p>
<p>交叉注意力</p>
<p>弱</p>
<p>中等</p>
<p>本文（Swin-T）</p>
<p>ViT patch token</p>
<p>注意力rollout+部分OT</p>
<p>强</p>
<p>高</p>
<h2 id="实验验证与结果分析">3. 实验验证与结果分析</h2>
<h3 id="实验设计">实验设计</h3>
<ul>
<li><strong>数据集</strong>：CUB200-2011（鸟类）、Cars196（汽车）、SOP（商品）、MSLS（街景定位）；</li>
<li><strong>评价指标</strong>：Precision@1（P@1）、R-Precision（RP）、MAP@R（M@R）、Recall@1；</li>
<li><strong>对比模型</strong>：DIML（ResNet-50）、Deit-S、CvT-13、Swin-T（base/struct版本）。</li>
</ul>
<h3 id="关键结果">关键结果</h3>
<h4 id="（1）vit变体对比">（1）ViT变体对比</h4>
<p>模型</p>
<p>CUB200 P@1（base/struct）</p>
<p>Cars196 P@1（base/struct）</p>
<p>SOP P@1（base/struct）</p>
<p>Deit-S</p>
<p>70.39/70.12（下降）</p>
<p>76.27/72.74（下降）</p>
<p>78.13/77.65（下降）</p>
<p>CvT-13</p>
<p>71.75/73.72（+1.97%）</p>
<p>80.55/83.66（+3.11%）</p>
<p>77.15/77.15（不变）</p>
<p>Swin-T</p>
<p><strong>74.47/74.98（+0.51%）</strong></p>
<p><strong>83.32/85.07（+1.75%）</strong></p>
<p><strong>79.42/80.02（+0.6%）</strong></p>
<p>结论：<strong>带卷积的ViT（CvT/Swin-T）才能从结构相似性中获益</strong>，纯ViT（Deit）因缺乏局部平滑特征导致效果下降。</p>
<h4 id="（2）部分ot效果">（2）部分OT效果</h4>
<p>SOP数据集上，部分OT（s=0.9s=0.9）比标准OT（s=1s=1）提升P@1 0.57%，证明其对强视角变化的鲁棒性。</p>
<h4 id="（3）街景定位（msls）">（3）街景定位（MSLS）</h4>
<p>方法</p>
<p>Recall@1（base/struct）</p>
<p>CvT-13（Distill）</p>
<p>60.68/66.35（+5.67%）</p>
<p>结论：结构重排序能显著提升视觉定位的精度，即使在蒸馏训练后仍有5%+的提升。</p>
<h2 id="未来研究方向与应用前景">4. 未来研究方向与应用前景</h2>
<h3 id="未来方向">未来方向</h3>
<ol>
<li><strong>动态部分OT参数</strong>：自动学习不同数据集的匹配比例ss，无需手动调整；</li>
<li><strong>跨模态扩展</strong>：将结构相似性应用于图文检索（如“这张图片的内容与您输入的文字描述中的‘红色连衣裙’相似”）；</li>
<li><strong>轻量化模型</strong>：压缩注意力rollout模块，使其适合边缘设备部署。</li>
</ol>
<h3 id="技术与投资机会">技术与投资机会</h3>
<ul>
<li><strong>技术机会</strong>：
<ul>
<li>边缘设备的<strong>轻量级图像检索SDK</strong>：整合结构重排序功能，无需云端计算；</li>
<li>电商平台的<strong>可视化推荐系统</strong>：用patch匹配图解释推荐理由；</li>
</ul>
</li>
<li><strong>投资机会</strong>：
<ul>
<li>专注于视觉检索的AI公司（如为电商提供精准推荐解决方案）；</li>
<li>自动驾驶视觉定位模块的供应商（如街景匹配算法优化）。</li>
</ul>
</li>
</ul>
<h2 id="论文不足与存疑之处">5. 论文不足与存疑之处</h2>
<ol>
<li><strong>纯ViT的适配性</strong>：Deit的结构版本效果下降，说明框架对纯ViT不友好，需进一步优化；</li>
<li><strong>参数手动调优</strong>：部分OT的匹配比例ss需手动设置，缺乏自适应机制；</li>
<li><strong>定量可解释性</strong>：仅通过定性可视化证明可解释性，缺乏定量指标（如“patch匹配准确率”）。</li>
</ol>
<h2 id="创新启发与背景知识补充">6. 创新启发与背景知识补充</h2>
<h3 id="创新启发">创新启发</h3>
<ol>
<li><strong>预训练ViT的patch利用</strong>：无需重新训练，直接用预训练ViT的patch token做结构重排序，降低应用成本；</li>
<li><strong>注意力rollout的新应用</strong>：将分类任务中的注意力可视化方法迁移到检索任务，量化patch重要性；</li>
<li><strong>部分OT的鲁棒性提升</strong>：针对特定场景（如强视角变化）定制最优传输策略，而非通用解决方案。</li>
</ol>
<h3 id="背景知识补充">背景知识补充</h3>
<ul>
<li><strong>最优传输（Optimal Transport）</strong>：数学上解决“如何将一个分布转换为另一个分布的最小成本”问题，在图像匹配中用于找到最优的patch对应关系；</li>
<li><strong>ViT架构</strong>：尤其是带卷积的变体（CvT/Swin-T）的结构设计，理解其局部特征学习机制；</li>
<li><strong>图像检索评价指标</strong>：Precision@1、R-Precision、MAP@R的定义与计算方式。</li>
</ul>
<p><strong>总结</strong>：本文是ViT在图像检索领域的重要突破，通过结构相似性+部分OT解决了精度、可解释性、鲁棒性三大痛点，为后续研究提供了清晰的方向。其“无需重新训练”的特性使其具有极高的产业落地价值。</p>
<p>图表代码</p>
<p>输入图像</p>
<p>ViT提取CLS+patch token</p>
<p>计算全局相似度（CLS）</p>
<p>注意力rollout生成patch重要性分布</p>
<p>计算结构相似度矩阵（patch）</p>
<p>D+E</p>
<p>部分最优传输匹配patch</p>
<p>C+F</p>
<p>结构重排序结果</p>
<p>输出Top-K检索结果+可视化解释</p>
<h1 id="《dynamicvit-efficient-vision-transformers-with-dynamic-token-sparsification》深度解读">《DynamicViT: Efficient vision transformers with dynamic token sparsification》深度解读</h1>
<h2 id="研究目标与问题意义-1">1. 研究目标与问题意义</h2>
<h3 id="研究目标-1">研究目标</h3>
<ul>
<li><strong>解决ViT计算效率瓶颈</strong>：Vision Transformer（ViT）的自注意力机制对所有token进行全局交互，导致高计算复杂度（如DeiT-S的4.6G FLOPs），难以部署在边缘设备上。</li>
<li><strong>提出动态token剪枝框架</strong>：基于输入图像的内容自适应剪枝冗余token，而非静态或结构化解码，保留关键信息的同时降低计算量。</li>
<li><strong>验证框架通用性</strong>：在DeiT、LV-ViT等主流ViT变体上验证动态剪枝的有效性，实现精度与效率的平衡。</li>
</ul>
<h3 id="问题意义">问题意义</h3>
<ul>
<li><strong>学术价值</strong>：首次系统探索<strong>动态token稀疏化</strong>（Dynamic Token Sparsification）在ViT中的应用，填补了自适应剪枝领域的空白。</li>
<li><strong>产业影响</strong>：剪枝后的ViT模型吞吐量提升40%以上，可直接部署于自动驾驶、手机端等资源受限场景，降低算力成本。</li>
</ul>
<h2 id="模型方法与创新思路-1">2. 模型方法与创新思路</h2>
<p>论文核心是<strong>分层动态token剪枝框架</strong>，包含预测模块、注意力掩码策略和多目标训练损失三部分。</p>
<h3 id="（1）预测模块：估计token重要性">（1）预测模块：估计token重要性</h3>
<p>预测模块用于生成每个token的保留/丢弃概率，公式如下：</p>
<ul>
<li><strong>局部特征提取</strong>：z_{\text{local}} = \text{MLP}(x) \in \mathbb{R}^{N \times C’}zlocal​=MLP(x)∈RN×C′（xx为token特征，C’=C/2C′=C/2）</li>
<li><strong>全局特征聚合</strong>：z_{\text{global}} = \text{Agg}(\text{MLP}(x), \hat{D}) = \frac{1}{N} \sum_{i=1}^N \hat{D}_i \cdot \text{MLP}(x_i)zglobal​=Agg(MLP(x),D<sup>)=N1​∑i=1N​D</sup>i​⋅MLP(xi​)（\hat{D}D^为历史决策掩码，Agg为平均池化）</li>
<li><strong>局部-全局特征融合</strong>：z_i = [z_{\text{local},i}, z_{\text{global}}]zi​=[zlocal,i​,zglobal​]</li>
<li><strong>概率预测</strong>：\pi = \text{Softmax}(\text{MLP}(z)) \in \mathbb{R}^{N \times 2}π=Softmax(MLP(z))∈RN×2（\pi_{i,1}πi,1​为保留概率）</li>
</ul>
<blockquote>
<p>“The local feature encodes the information of a certain token while the global feature contains the context of the whole image, thus both of them are informative.”</p>
</blockquote>
<h3 id="（2）注意力掩码：实现微分剪枝">（2）注意力掩码：实现微分剪枝</h3>
<p>为解决离散剪枝的不可微分问题，论文采用：</p>
<ul>
<li>
<p><strong>Gumbel-Softmax采样</strong>：D = \text{Gumbel-Softmax}(\pi)_{:,1}D=Gumbel-Softmax(π):,1​（生成可微分的二进制掩码）</p>
</li>
<li>
<p><strong>注意力矩阵阻塞</strong>：在自注意力计算中，将丢弃token的注意力权重置零，公式为：</p>
<p>\tilde{A}<em>{ij} = A</em>{ij} \cdot \hat{D}_i \cdot \hat{D}_jA~ij​=Aij​⋅D<sup>i​⋅D</sup>j​</p>
<p>（\hat{D}D^为更新后的决策掩码，\hat{D} \leftarrow \hat{D} \odot DD<sup>←D</sup>⊙D）</p>
</li>
</ul>
<h3 id="（3）多目标训练损失">（3）多目标训练损失</h3>
<p>总损失包含四部分：</p>
<p>L = L_{\text{cls}} + \lambda_{\text{KL}}L_{\text{KL}} + \lambda_{\text{distill}}L_{\text{distill}} + \lambda_{\text{ratio}}L_{\text{ratio}}L=Lcls​+λKL​LKL​+λdistill​Ldistill​+λratio​Lratio​</p>
<ul>
<li>L_{\text{cls}}Lcls​：分类交叉熵损失</li>
<li>L_{\text{KL}}LKL​：与教师模型（原ViT）的预测分布KL散度</li>
<li>L_{\text{distill}}Ldistill​：token级特征蒸馏损失</li>
<li>L_{\text{ratio}}Lratio​：剪枝比例约束（MSE损失，确保剪枝率符合目标）</li>
</ul>
<h3 id="框架流程图">框架流程图</h3>
<p>图表代码</p>
<p>输入图像</p>
<p>Patch Embedding</p>
<p>Transformer Block1-3</p>
<p>预测模块1</p>
<p>动态剪枝token</p>
<p>Transformer Block4-6</p>
<p>预测模块2</p>
<p>动态剪枝token</p>
<p>Transformer Block7-9</p>
<p>预测模块3</p>
<p>动态剪枝token</p>
<p>Transformer Block10-12</p>
<p>分类头</p>
<p>输出结果</p>
<h2 id="实验验证与结果分析-1">3. 实验验证与结果分析</h2>
<h3 id="关键数据">关键数据</h3>
<ul>
<li><strong>DeiT-S剪枝效果</strong>：剪枝66%token后，FLOPs从4.6G降至2.9G（-37%），吞吐量提升54%（1337→2062 im/s），精度仅降0.5%（79.8→79.3%）。</li>
<li><strong>SOTA对比</strong>：DynamicViT-LV-M/0.7（8.5G FLOPs）精度83.8%，超过EfficientNet-B5（9.9G FLOPs，83.6%）。</li>
<li><strong>跨模型通用性</strong>：LV-ViT-S剪枝后，FLOPs降31%，吞吐量提升43%，精度降0.3%。</li>
</ul>
<h3 id="核心结论">核心结论</h3>
<p>动态token剪枝在保持精度的同时，显著提升ViT的计算效率，优于静态剪枝或结构化解码方法。</p>
<h2 id="未来研究方向与应用前景-1">4. 未来研究方向与应用前景</h2>
<h3 id="未来方向-1">未来方向</h3>
<ul>
<li><strong>扩展到下游任务</strong>：将动态剪枝应用于目标检测（DETR）、语义分割等任务，探索token剪枝对密集预测的影响。</li>
<li><strong>优化预测模块</strong>：设计更轻量化的预测模块，进一步降低额外计算开销。</li>
<li><strong>多模态动态剪枝</strong>：将框架扩展到图文跨模态模型（如CLIP），实现跨模态token的自适应剪枝。</li>
</ul>
<h3 id="应用前景">应用前景</h3>
<ul>
<li><strong>自动驾驶</strong>：剪枝后的ViT可实时处理车载摄像头图像，提升目标检测速度。</li>
<li><strong>边缘设备部署</strong>：DynamicViT-S可在手机端实现高精度图像分类，功耗降低30%以上。</li>
</ul>
<h2 id="论文不足与存疑之处-1">5. 论文不足与存疑之处</h2>
<ul>
<li><strong>小剪枝率时精度下降</strong>：当剪枝率超过70%（ρ&lt;0.3），精度下降明显（如DeiT-S ρ=0.5时精度降2.3%）。</li>
<li><strong>预测模块开销</strong>：预测模块引入约5%的额外计算，在极端资源受限场景下需进一步优化。</li>
<li><strong>跨数据集泛化性</strong>：实验主要基于ImageNet，在医疗影像、遥感图像等领域的泛化性待验证。</li>
</ul>
<h2 id="创新启发与背景知识补充-1">6. 创新启发与背景知识补充</h2>
<h3 id="创新启发-1">创新启发</h3>
<ul>
<li><strong>动态剪枝思路</strong>：自适应剪枝比静态剪枝更高效，可迁移到NLP Transformer模型。</li>
<li><strong>微分剪枝方法</strong>：注意力掩码结合Gumbel-Softmax，为离散决策的端到端训练提供新范式。</li>
<li><strong>分层剪枝策略</strong>：逐步剪枝token，保留中间层的关键信息，平衡精度与效率。</li>
</ul>
<h3 id="背景知识补充-1">背景知识补充</h3>
<ul>
<li><strong>ViT基础</strong>：需掌握Patch Embedding、自注意力机制、Class Token等核心组件。</li>
<li><strong>Gumbel-Softmax</strong>：了解其原理，用于处理离散采样的微分问题。</li>
<li><strong>知识蒸馏</strong>：熟悉教师-学生模型的训练方法，理解token级蒸馏的实现。</li>
</ul>
<p><strong>总结</strong>：DynamicViT通过动态token剪枝，有效解决了ViT的计算效率问题，为Transformer模型的部署提供了新的思路。其核心创新点在于自适应剪枝策略和微分训练方法，具有重要的学术和产业价值。</p>

